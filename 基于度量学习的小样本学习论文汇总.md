## 基于度量学习的小样本学习
--- 
### 1. Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning
[论文地址](https://arxiv.org/abs/1903.12290?source=post_page)

![](https://github.com/wangyao049/-/blob/master/image/1.png)

- #### 算法思想：本文提出了一种使用基于局部描述子的image-to-class度量方法。基于NBNN(朴素贝叶斯最近邻算法)提出了DN4（Deep Nearest Neighbor Neural Network）模型,该模型分为两部分：
    - (1) **深度嵌入模型**，用来学习所有图像的深度局部描述子。该模型可以是任何CNN结构，只有卷积层，没有全连接层（因为提取的是局部特征，不是全局特征）,最后每张图片输出一张局部特征图（h\*w\*d tensor）; 
    - (2) **Image-to-class模型**，根据生成的局部特征，使用KNN计算query image和各种类别之间的image-to-class相似度（距离）。计算方法为，对于每张图片提取到的局部特征向量图q=[x1,x2,....,xm]中的每一个描述子xi,都在class c中找到其对应的 k-nearest neighbors，然后计算他们的相似度并累加，得到最后的image-to-class similarity between q and class c。（这里的相似度方程可以使用余弦相似度或者其他的距离方程）。相对于常规的图像分类问题，image-to-class的度量方法更适用于小样本分类，因为对大量数据搜索k近邻时计算复杂度很高。同时相对于传统的NBNN算法，本文使用CNN来提取局部特征而不是手工提取的特征。（这部分是没有可学习参数的，因此可以减轻过拟合问题）
    
- #### 问题：(1)为什么使用局部描述子(local descriptor)代替全局特征(image-level feature)？
    - **Image-level representation could significantly lose discriminative information.** If there are sufficient training samples, the subsequent learning process can somehow recover form  such a loss, still showing satisfactory classification performance. But, **when training samples are insufficient, this loss is unrecoveralbe and leads to poor classificaiton.** The existing methods usually pool the last convolutional feature maps(via the global average pooling or fully connected layer)to an image-level representaion for the final calssification. In this case, such an information loss will also occur and is unrecoverable.

- #### 问题：(2)为什么使用image-to-class相似度而不是image-to-image相似度？
    - This is because **image-to-image similarity does not generalize beyond training samples.** When the number of training samples is small, **a query image could be different from any training samples of the same class due to intra-class variation or background clutter.** Instead, and image-to-class measure should be used. 
    - Specifically, the local invariant features from all training samples in the same class are collected into one pool(属于同一个类别的所有训练数据的特征都放在一个池里). This measure evaluates the proximity(via nearest-neighbor search)of the local features of a query image to the pool of each class for classification(通过k近邻搜索找到与query image的特征最相近的类别池进行分类).
-----
### 2. Few-Shot Learning with Localization in Realistic Settings
[论文地址](https://arxiv.org/abs/1904.08502?source=post_page)

![](https://github.com/wangyao049/-/blob/master/2.png)

- #### 算法思想：传统的小样本学习方法使用的测试集均采用人为设定的均衡的数据样本，但是在真实环境下，存在数据不均衡，背景模糊以及细粒度图像等问题。因此，本文针对真实环境下的数据不平衡(class imbalance)，数据稀疏(data scarcity)，重尾分布(heavy-tailed distributions)等问题，提出了一种基于原型网络(Prototypical networks)的改进算法，从三个方面对原型网络进行改进。

- #### 改进：(1)为了解决数据不平衡问题，使用leave-one-out cross-validation的训练方法。(Batch Folding)
    - 传统方法是将每个batch中的图片划分为reference/query,进行训练。这里使用了留一法交叉验证，每一张图片都既是reference又是query。该方法提高了4个百分点准确率。
    - The entire batch is treated as reference images, and the contribution of each image is subtracted out from its corresponding prototype whenever it acts as a query. **Each image thus gets a combined, cleaner gradient, acting as both a reference and a query.** Furthermore, the number of query/reference images can be as high as the batch size.
    
- #### 改进：(2)为了解决当图片中前景过小，以及






